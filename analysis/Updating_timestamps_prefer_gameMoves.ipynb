{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-16T13:53:37.328499Z",
     "start_time": "2025-12-16T13:53:37.261823Z"
    }
   },
   "source": [
    "## ðŸš€ Setup and Helper Functions\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- Helper function for half-second window calculation ---\n",
    "\n",
    "def get_half_second_target(ts_datetime):\n",
    "    \"\"\"\n",
    "    Calculates the nearest .000 or .500 target time for a given datetime,\n",
    "    used for rounding targets and defining 500ms windows.\n",
    "    \"\"\"\n",
    "    milliseconds = ts_datetime.microsecond // 1000\n",
    "    remainder = milliseconds % 500\n",
    "\n",
    "    if remainder < 250:\n",
    "        milliseconds -= remainder\n",
    "    else:\n",
    "        milliseconds += (500 - remainder)\n",
    "\n",
    "    if milliseconds == 1000:\n",
    "        # Handle overflow to the next second\n",
    "        target = ts_datetime + timedelta(seconds=1)\n",
    "        target = target.replace(microsecond=0)\n",
    "    else:\n",
    "        target = ts_datetime.replace(microsecond=milliseconds * 1000)\n",
    "\n",
    "    return target\n",
    "\n",
    "# --- Helper function for 100ms rounding ---\n",
    "\n",
    "def round_to_100ms(ts_datetime):\n",
    "    \"\"\"\n",
    "    Rounds a datetime object to the nearest 100 milliseconds (one decimal of a second).\n",
    "    \"\"\"\n",
    "    total_milliseconds = (ts_datetime.microsecond // 1000)\n",
    "\n",
    "    # Round the milliseconds to the nearest multiple of 100\n",
    "    rounded_milliseconds = round(total_milliseconds / 100) * 100\n",
    "\n",
    "    # Handle overflow from 999ms to 1000ms (which is 1 second)\n",
    "    if rounded_milliseconds == 1000:\n",
    "        dt_rounded = ts_datetime + timedelta(seconds=1)\n",
    "        dt_rounded = dt_rounded.replace(microsecond=0)\n",
    "    else:\n",
    "        # Update the microsecond part\n",
    "        dt_rounded = ts_datetime.replace(microsecond=rounded_milliseconds * 1000)\n",
    "\n",
    "    return dt_rounded\n",
    "\n",
    "\n",
    "def final_unique_resolution(all_moves_list):\n",
    "    \"\"\"\n",
    "    Performs a final chronological pass to ensure all timestamps are strictly\n",
    "    increasing, using a 100ms increment for collisions. This is the last step.\n",
    "    \"\"\"\n",
    "    # Sort by the tentatively assigned final_ts_dt\n",
    "    all_moves_list.sort(key=lambda x: x['final_ts_dt'])\n",
    "\n",
    "    previous_final_ts = None\n",
    "\n",
    "    for move in all_moves_list:\n",
    "        final_ts = move['final_ts_dt']\n",
    "\n",
    "        # Chronological uniqueness check: push it past the previous move\n",
    "        if previous_final_ts and final_ts <= previous_final_ts:\n",
    "            final_ts = previous_final_ts + timedelta(milliseconds=100)\n",
    "\n",
    "        move['final_ts_dt'] = final_ts\n",
    "        previous_final_ts = final_ts\n",
    "\n",
    "    return all_moves_list\n",
    "\n",
    "\n",
    "def update_timestamps(data):\n",
    "    \"\"\"\n",
    "    Applies the multi-stage, prioritized, and filtered timestamp update process.\n",
    "    \"\"\"\n",
    "\n",
    "    original_game_moves = data['gameMoves']\n",
    "    original_do_something_timestamps = data['doSomethingTimestamps']\n",
    "\n",
    "    final_timestamps_set = set() # To track the assigned slots (only for half-second check)\n",
    "\n",
    "    # --- 1. Process gameMoves (100ms round, then .0/.5 priority) ---\n",
    "    processed_gm = []\n",
    "    sorted_gm_items = sorted(original_game_moves.items())\n",
    "\n",
    "    for ts_str, value in sorted_gm_items:\n",
    "        original_dt = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))\n",
    "\n",
    "        # 1a. Round to the nearest 100ms\n",
    "        dt_100ms = round_to_100ms(original_dt)\n",
    "\n",
    "        # 1b. Check for closest .0 or .5 target\n",
    "        target_half_second = get_half_second_target(dt_100ms)\n",
    "\n",
    "        assigned_ts = dt_100ms # Default to the 100ms rounded time\n",
    "\n",
    "        # Priority rule: Round to .0 or .5 if available\n",
    "        if target_half_second not in final_timestamps_set:\n",
    "            assigned_ts = target_half_second\n",
    "\n",
    "        processed_gm.append({\n",
    "            'final_ts_dt': assigned_ts,\n",
    "            'type': 'gameMoves',\n",
    "            'value': value\n",
    "        })\n",
    "        # Tentative assignment added to the set to block lower priority moves\n",
    "        final_timestamps_set.add(assigned_ts)\n",
    "\n",
    "    # --- 2. Process doSomethingTimestamps (100ms round, then 500ms filtering) ---\n",
    "    filtered_dst = []\n",
    "    half_second_windows = set() # Tracks the 500ms windows that already have a move\n",
    "    sorted_dst_items = sorted(original_do_something_timestamps.items())\n",
    "\n",
    "    for ts_str, value in sorted_dst_items:\n",
    "        original_dt = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))\n",
    "\n",
    "        # 2a. Round to the nearest 100ms\n",
    "        dt_100ms = round_to_100ms(original_dt)\n",
    "\n",
    "        # 2b. Determine the 500ms window key (e.g., 33:35.000)\n",
    "        window_dt = get_half_second_target(dt_100ms)\n",
    "\n",
    "        # Filtering rule: Keep only the first move in this 500ms window\n",
    "        if window_dt not in half_second_windows:\n",
    "            filtered_dst.append({\n",
    "                'final_ts_dt': dt_100ms, # Assigned time is the 100ms rounded value\n",
    "                'type': 'doSomethingTimestamps',\n",
    "                'value': value\n",
    "            })\n",
    "            half_second_windows.add(window_dt)\n",
    "\n",
    "    # 3. Combine for final resolution\n",
    "    all_moves_list = processed_gm + filtered_dst\n",
    "\n",
    "    # 4. Final Collision Resolution and Timestamp Assignment (Enforce global uniqueness)\n",
    "    resolved_moves_list = final_unique_resolution(all_moves_list)\n",
    "\n",
    "    # 5. Final Rebuild of Dictionaries\n",
    "    new_game_moves = {}\n",
    "    new_do_something_timestamps = {}\n",
    "\n",
    "    for move in resolved_moves_list:\n",
    "        final_ts_str = move['final_ts_dt'].isoformat().replace('+00:00', 'Z')\n",
    "\n",
    "        if move['type'] == 'gameMoves':\n",
    "            new_game_moves[final_ts_str] = move['value']\n",
    "        elif move['type'] == 'doSomethingTimestamps':\n",
    "            new_do_something_timestamps[final_ts_str] = move['value']\n",
    "\n",
    "    data['gameMoves'] = new_game_moves\n",
    "    data['doSomethingTimestamps'] = new_do_something_timestamps\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Main processing function (remains the same) ---\n",
    "\n",
    "def process_files(file_list, output_prefix='_updated'):\n",
    "    \"\"\"\n",
    "    Processes a list of JSON files to update timestamps and saves the\n",
    "    result to a new file with the specified prefix.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting file processing for {len(file_list)} files ---\")\n",
    "\n",
    "    for input_filepath in file_list:\n",
    "        try:\n",
    "            # 1. Determine the output filename\n",
    "            directory = os.path.dirname(input_filepath)\n",
    "            filename_with_ext = os.path.basename(input_filepath)\n",
    "            name, ext = os.path.splitext(filename_with_ext)\n",
    "\n",
    "            # Construct the new filename: insert '_updated' before the final number\n",
    "            last_underscore_index = name.rfind('_')\n",
    "            if last_underscore_index != -1 and name[last_underscore_index+1:].isdigit():\n",
    "                base_name = name[:last_underscore_index]\n",
    "                suffix = name[last_underscore_index:]\n",
    "                new_name = f\"{base_name}{output_prefix}{suffix}\"\n",
    "            else:\n",
    "                new_name = f\"{name}{output_prefix}\"\n",
    "\n",
    "            output_filepath = os.path.join(directory, f\"{new_name}{ext}\")\n",
    "\n",
    "            print(f\"\\nProcessing: {input_filepath}\")\n",
    "\n",
    "            # 2. Load the data\n",
    "            with open(input_filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Check original lengths before update\n",
    "            original_gm_len = len(data['gameMoves'])\n",
    "            original_dst_len = len(data['doSomethingTimestamps'])\n",
    "            total_original = original_gm_len + original_dst_len\n",
    "\n",
    "            # 3. Update the timestamps\n",
    "            updated_data = update_timestamps(data)\n",
    "\n",
    "            # Check final lengths after update\n",
    "            final_gm_len = len(updated_data['gameMoves'])\n",
    "            final_dst_len = len(updated_data['doSomethingTimestamps'])\n",
    "            total_final = final_gm_len + final_dst_len\n",
    "\n",
    "            # Verify that GameMoves count is preserved (DST count is expected to drop due to filtering)\n",
    "            if original_gm_len != final_gm_len:\n",
    "                raise ValueError(\n",
    "                    f\"DATA MISMATCH: Original GameMoves count ({original_gm_len}) \"\n",
    "                    f\"does not match final GameMoves count ({final_gm_len}). \"\n",
    "                    \"DST count reduction is expected but GM count must be preserved.\"\n",
    "                )\n",
    "\n",
    "            # 4. Save the new data\n",
    "            with open(output_filepath, 'w') as f:\n",
    "                json.dump(updated_data, f, indent=4)\n",
    "\n",
    "            print(f\"Output to: {output_filepath}\")\n",
    "            print(f\"Successfully updated and saved. Total original moves: {total_original}, final moves: {total_final}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            cleaned_path = input_filepath.replace('\\\\', '/')\n",
    "            print(f\"!!! FATAL ERROR processing {cleaned_path}: {e}\")\n",
    "\n",
    "# --- Execution Cell ---\n",
    "\n",
    "# Define the pattern to find all relevant JSON files.\n",
    "# NOTE: If you run this in a different environment, you may need to adjust the file_pattern\n",
    "# or the directory structure for files to be found.\n",
    "file_pattern = '../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)/*.json'\n",
    "\n",
    "# Get the list of all files to process\n",
    "all_files_to_process = glob.glob(file_pattern)\n",
    "\n",
    "# Execute the main function\n",
    "process_files(all_files_to_process)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting file processing for 11 files ---\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_1.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_1.json\n",
      "Successfully updated and saved. Total original moves: 472, final moves: 383\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_10.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_10.json\n",
      "Successfully updated and saved. Total original moves: 640, final moves: 426\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_2.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_2.json\n",
      "Successfully updated and saved. Total original moves: 964, final moves: 417\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_3.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_3.json\n",
      "Successfully updated and saved. Total original moves: 557, final moves: 446\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_4.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_4.json\n",
      "Successfully updated and saved. Total original moves: 444, final moves: 386\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_5.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_5.json\n",
      "Successfully updated and saved. Total original moves: 520, final moves: 459\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_6.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_6.json\n",
      "Successfully updated and saved. Total original moves: 185, final moves: 182\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_7.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_7.json\n",
      "Successfully updated and saved. Total original moves: 591, final moves: 442\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_8.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_8.json\n",
      "Successfully updated and saved. Total original moves: 517, final moves: 438\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_9.json\n",
      "Output to: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_updated_9.json\n",
      "Successfully updated and saved. Total original moves: 511, final moves: 454\n",
      "\n",
      "Processing: ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)\\Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_overview.json\n",
      "!!! FATAL ERROR processing ../output/all/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)/Two, one and one (2 SHORT_FUSE, 1 BASIC, 1 REACTIVE)_overview.json: 'gameMoves'\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
